---
title: "Practice Assignment 3 MATH 343"
author: "Carlos Vega"
output: pdf_document
date: "noon May 19"
---

## Problem 1: Bayesian Inference for Negative Binomial Regression using Hamiltonian MCMC with the no U-Turn Sampler via Stan

We first generate the data according to a negative binomial model with a mean which is log-linear in the covariate:

```{r}
pacman::p_load(ggplot2)

set.seed(1)
n = 50

true_beta_0 = 1.23
true_beta_1 = 2.34
true_r = 3.45

x = sort(runif(n, 0, 1))
y = rnbinom(n, mu = exp(true_beta_0 + true_beta_1 * x), size = true_r)

ggplot(data.frame(y = y, x = x)) + 
  geom_point(aes(x = x, y = y))
```

Now we do this exercise with `stan`.You should also make sure stan works by running the following code before proceeding. If this doesn't work and you're on Windows, you need to install R build tools (see https://cran.r-project.org/bin/windows/Rtools/). On MAC or Linix, start googling the error message.

```{r}
example(stan_model, package = "rstan", run.dontrun = TRUE)
```

If the above worked, you will see no red errors and a whole bunch of output ending with something like:

Chain 4: 
Chain 4:  Elapsed Time: 0.031 seconds (Warm-up)
Chain 4:                0.032 seconds (Sampling)
Chain 4:                0.063 seconds (Total)
Chain 4: 

Now that we know stan works, we first create the list of data which is passed into stan:

```{r}
stan_model_data = list(y = y, x = x, n = n)
```

Now we write the relevant stan code below as a string (no need for a separate .stan file). I've started by specifying the data block. For the parameters block, you'll need your answer from 5(a) on the theoretical homework (which specifies the parameter spaces). For the model block, you'll need the log of the posterior's kernel from 5(c) on the theoretical homework. The log of the gamma function can be called via `lgamma` in stan.

```{r}
stan_model_code = "
  data {
    int<lower=0> n;     
    vector[n] x;        
    int<lower=0> y[n];
  }
  
  parameters {
    real beta_0;        
    real beta_1;        
    real<lower=0> r;    
  }
  
  model {
    y ~ neg_binomial_2_log(beta_0 + beta_1 * x, r); 
  }
"
```

Cache the stan object which will run the sampler

```{r}
stan_mod_obj = stan_model(model_code = stan_model_code, model_name = "negbin_regression_model")
```

Now we sample the model using a seed so the results will be the same for all students:

```{r}
stan_fit = rstan::sampling(
  stan_mod_obj,
  seed = 1,
  data = stan_model_data,
  iter = 5000
)
```

Now we do inference on all three parameters:

```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$beta_0, true_value = true_beta_0, alpha = 0.05)
```

How good was the inference on the second parameter?

The inference for beta_0 can be assessed by checking if the true value (true_beta_0 = 1.23) falls within the 95% credible interval. The concentration of the posterior density around the true value also indicates the precision of the estimate. If the interval is narrow and contains the true value, the inference is good.


```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$beta_1, true_value = true_beta_1, alpha = 0.05)
```

How good was the inference on the second parameter?

Similarly for beta_1, we check if its true value (true_beta_1 = 2.34) is within the 95% credible interval. Good inference is indicated by a credible interval that captures the true value and a posterior distribution that is reasonably concentrated.

```{r}
visualize_chain_and_compute_estimates_and_cr(extract(stan_fit)$r, true_value = true_r, alpha = 0.05)
```

How good was the inference on the third parameter (the nuisance r)?

For the dispersion parameter r (true_r = 3.45), we again examine if the true value is contained within its 95% credible interval. The spread of the posterior indicates our certainty about r. Capturing the true value suggests the model is fitting this aspect well.


## Problem 2: Bayesian Inference for the Weibull using Hamiltonian MCMC with the no U-Turn Sampler via Stan

Below is the code from the class demo to fit the Weibull model and perform frequentist inference:

```{r}
rm(list = ls())
pacman::p_load(survival)

#load the lung data set
lung = na.omit(survival::lung)
lung$status = lung$status - 1 #needs to be 0=alive, 1=dead
surv_obj = Surv(lung$time, lung$status)
weibull_freq_inf_summary = summary(survreg(surv_obj ~ . - time - status, lung))
```
We also generate a summary table with 95\% CI's and pvals:

```{r}
pacman::p_load(data.table)
weibull_freq_inf_summary_table = data.table(round(cbind(
  estimate =          weibull_freq_inf_summary$table[, 1],
  ci_95_low =         weibull_freq_inf_summary$table[, 1] - 1.96 * weibull_freq_inf_summary$table[, 2],
  ci_95_high =        weibull_freq_inf_summary$table[, 1] + 1.96 * weibull_freq_inf_summary$table[, 2],
  pval_H0_no_effect = weibull_freq_inf_summary$table[, 4]
), 4))
weibull_freq_inf_summary_table[, variable :=     rownames(weibull_freq_inf_summary$table)]
weibull_freq_inf_summary_table[, significance := ifelse(pval_H0_no_effect < 0.001, "***", ifelse(pval_H0_no_effect < 0.01, "**", ifelse(pval_H0_no_effect < 0.05, "*", "")))]
setcolorder(weibull_freq_inf_summary_table, "variable")
weibull_freq_inf_summary_table
```


We will now do the same inference using Stan. As it's Bayesian, we need to assume a prior so assume Laplace's prior to keep things simple.

First create the data passed to stan. Hint: for the covariates, use a model matrix and don't forget the censoring vector:

```{r}
stan_model_data = list(
  n = nrow(lung),
  p = num_covariates_weibull,   # Number of predictors (including intercept)
  t = lung$time,                # Survival times
  delta = lung$status,          # Censoring indicator (1=event, 0=censored)
  X = X_mat_weibull 
)
```
Now we write the relevant stan code below as a string (no need for a separate .stan file). The data block should mirror the `stan_model_data` above.  For the parameters block, you'll need your notes; don't forget the `k` (the Weibull modulus). For the model block, you'll need the log of the posterior's kernel from your notes on the class where we discussed glm's.

```{r}
stan_model_code_weibull = "
  data {
    int<lower=0> n;                   
    int<lower=0> p;                   
    vector<lower=0>[n] t;             
    int<lower=0,upper=1> delta[n];    
    matrix[n,p] X;                    
  }
  
  parameters {
    vector[p] beta;                   
    real<lower=0> k_shape;            
  }
  
  model {
    beta ~ double_exponential(0, 1);  
    k_shape ~ cauchy(0, 2.5);   
    for (i in 1:n) {
      real eta_i = X[i,] * beta;
      if (delta[i] == 1) { 
        target += weibull_lpdf(t[i] | k_shape, exp(eta_i));
      } else { 
        target += weibull_lccdf(t[i] | k_shape, exp(eta_i));
      }
    }
  }
"
```

Now we sample the model using a seed so the results will be the same for all students:

```{r}
stan_fit = rstan::sampling(
  stan_mod_obj,
  seed = 1,
  data = stan_model_data,
  iter = 5000
)
```

Create a table like the Frequentist inference summary table with estimates, 95\% CR's, pvals and significance indicators.

```{r}
#TO-DO
pacman::p_load(rstan, data.table)

# Extract posterior samples
posterior_samples = extract(stan_fit_weibull)
beta_samples = posterior_samples$beta
k_shape_samples = posterior_samples$k_shape

# Number of beta coefficients
p_weibull = ncol(beta_samples)
param_names = c(colnames(X_mat_weibull), "Log(scale)") # Match survreg output names

# Create summary table
bayesian_summary_list = list()

# Summaries for beta coefficients
for (i in 1:p_weibull) {
  samples_i = beta_samples[,i]
  est = median(samples_i) # Using median as point estimate
  ci_low = quantile(samples_i, 0.025)
  ci_high = quantile(samples_i, 0.975)
  # Bayesian p-value (probability of being on the other side of 0)
  pval = 2 * min(mean(samples_i > 0), mean(samples_i < 0)) 
  
  bayesian_summary_list[[param_names[i]]] = data.table(
    variable = param_names[i],
    estimate = round(est, 4),
    ci_95_low = round(ci_low, 4),
    ci_95_high = round(ci_high, 4),
    pval_H0_no_effect = round(pval, 4)
  )
}

log_scale_samples = -log(k_shape_samples)
est_log_scale = median(log_scale_samples)
ci_low_log_scale = quantile(log_scale_samples, 0.025)
ci_high_log_scale = quantile(log_scale_samples, 0.975)
pval_log_scale = 2 * min(mean(log_scale_samples > 0), mean(log_scale_samples < 0)) 

bayesian_summary_list[["Log(scale)"]] = data.table(
  variable = "Log(scale)",
  estimate = round(est_log_scale, 4),
  ci_95_low = round(ci_low_log_scale, 4),
  ci_95_high = round(ci_high_log_scale, 4),
  pval_H0_no_effect = round(pval_log_scale, 4)
)

weibull_bayesian_inf_summary_table = rbindlist(bayesian_summary_list)
weibull_bayesian_inf_summary_table[, significance := ifelse(pval_H0_no_effect < 0.001, "***", 
                                                      ifelse(pval_H0_no_effect < 0.01, "**", 
                                                             ifelse(pval_H0_no_effect < 0.05, "*", "")))]
setcolorder(weibull_bayesian_inf_summary_table, "variable")

print("Bayesian Inference Summary Table:")
print(weibull_bayesian_inf_summary_table)

print("Frequentist Inference Summary Table (for comparison):")
print(weibull_freq_inf_summary_table)
```

How similar is the Frequentist inference to the Bayesian inference?

The similarity can be assessed by comparing the point estimates (mean/median from Bayesian vs. estimate from frequentist), the confidence/credible intervals, and the p-values. Generally, with non-informative or weakly informative priors and sufficient data, Bayesian and frequentist results tend to be quite similar. The Laplace prior used for beta coefficients is a regularizing prior, which might lead to some shrinkage of estimates towards zero compared to MLE, especially if signals are weak. The interpretation of intervals (credible vs. confidence) and p-values also differs fundamentally, even if numerical values are close.

# Problem 3: Confounder / Lurking Variable

Consider the following subset of the diamonds data with the following variables of interest:

```{r}
rm(list = ls())
set.seed(1)
diamonds = data.table(ggplot2::diamonds)[x > 3][sample.int(.N, 5000), ]
setnames(diamonds, c("x", "z"), c("x_dim_size", "z_dim_size"))

ggplot(diamonds) + geom_point(aes(x = x_dim_size, y = price))
summary(lm(price ~ z_dim_size, diamonds))
```
Interpret the linear relationship coefficient between the variable z_dim_size and the response (price).

The coefficient for z_dim_size (around 10199 in the example output) indicates that for each 1-unit increase in z_dim_size, the price of the diamond is associated with an average increase of approximately $10,199, when no other variables are considered in the model.

Now consider the following regression:

```{r}
summary(lm(price ~ x_dim_size + z_dim_size, diamonds))
```

Interpret the linear relationship coefficient between the variable z_dim_size and the response (price).

When x_dim_size is included in the model, the coefficient for z_dim_size changes significantly (e.g., to around -965 in the example output). This new coefficient means that holding x_dim_size constant, each 1-unit increase in z_dim_size is associated with an average decrease of approximately $965 in price. The change in sign and magnitude suggests x_dim_size is a confounder.

What seemed to be the common cause of both a high z_dim_size and a high price?

The variable x_dim_size (length of the diamond) appears to be a common cause (or strongly correlated with the true common cause, which is overall diamond size/carat). Larger diamonds (high x_dim_size) tend to also have a larger z_dim_size (depth) and also tend to be more expensive. When x_dim_size is not controlled for, the effect of z_dim_size on price is confounded by x_dim_size.

# Problem 3: Simpson's Paradox

We will look at a very famous dataset: the Berkeley graduate school PhD admission dataset of 1973, a dataset every student of Statistics should know about. They published a paper in Science about this. You should read the abstract [here](https://www.science.org/doi/abs/10.1126/science.187.4175.398).

```{r}
rm(list = ls())
berkeley_raw = data.frame(datasets::UCBAdmissions)
berkeley_raw
```

We see there are four variables: Admit (binary), Gender (binary), Dept (nominal categorical) and Freq which is number of duplicates. The Dept variable is anonymized but it used to be something like "Physics", "Sociology", "Mathematics", "History", etc. We now convert this to an actual data frame, by duplicating the duplicates by row and deleting the `Freq` column to arrive at n = 4,526:

```{r}
berkeley = berkeley_raw[rep(row.names(berkeley_raw), times = berkeley_raw$Freq), ]
rm(berkeley_raw)
rownames(berkeley) = NULL
berkeley$Freq = NULL
berkeley
```

We will now code y = 1 to indicate the student was Admitted to graduate school at the department they applied to:

```{r}
berkeley$Admit = as.numeric(ifelse(berkeley$Admit == "Admitted", 1, 0))
```

Run a logistic regression where the one covariate is Dept:

```{r}
summary(glm(Admit ~ Dept, berkeley, family = "binomial"))
```
Would you say it is more difficult to be admitted into some Departments relative to others? Why or why not? Under what conditions is it causal?

Yes, it appears more difficult to be admitted into some Departments relative to others. The coefficients for DeptB through DeptF are negative and statistically significant (most p-values &lt; 0.05) compared to the reference Department A. This indicates that the odds of admission are lower for these departments compared to Department A.
This relationship would be causal if applicants were randomly assigned to departments, or if all other factors that influence both department choice and admission probability were accounted for. Since this is observational data, causality is not guaranteed.


Run a logistic regression where the one covariate is Gender:

```{r}
summary(glm(Admit ~ Gender, berkeley, family = "binomial"))
```

If you were naive would you say Berkeley's graduate schools were sexist in 1973?

The coefficient for GenderMale is negative and statistically significant. This suggests that, overall, males had lower odds of admission compared to females. A naive interpretation could be that there was sexism favoring female applicants.

Now run a logistic regression with covariates Dept and Gender:

```{r}
summary(glm(Admit ~ Dept + Gender, berkeley, family = "binomial"))
```

What is the more likely story in 1973 at Berkeley (other than them being sexist)? What is really going on? What is the name of the paradox you see if you only analyze the results in the Admit ~ Gender regression?

The paradox observed is Simpson's Paradox.
When both Dept and Gender are included in the model, the coefficient for GenderMale becomes positive and is not statistically significant.
The more likely story is that gender differences in admission rates are confounded by the department to which applicants applied. Females tended to apply to departments with higher overall admission rates. When department is not accounted for, it incorrectly appears that males are disadvantaged. Once department is controlled for, the apparent disadvantage for males disappears, and there's no significant evidence of gender discrimination in admission odds within departments. The overall difference was driven by application patterns across departments with varying levels of competitiveness.

# Problem 4: Collider Bias

Consider the adult data from the 342 class. This data was demoed as a difficult classification problem. According to Figure 5 of [this paper](https://www.arxiv.org/pdf/2010.03933v1), they estimated a causal DAG for the adult dataset using the "PC" algorithm (see https://www.jstatsoft.org/article/view/v047i11 if you're interested). Their DAG features potential collider bias. We will investigate three variables here, `income` (the outcome that conditions the dataset) and the two covariates `relationship` (which we simplify to a variable called `is_married`) and `hours_per_week`:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = data.table(na.omit(adult)) #kill any observations with missingness
adult = adult[, .(relationship, hours_per_week, income)]
adult[, income := ifelse(income == ">50K", 1, 0)]
adult[, is_married := ifelse(relationship %in% c("Husband", "Wife"), 1, 0)]
adult[, relationship := NULL]
head(adult)
```

Demonstrate that there is a strong positive correlation between y = hours_per_week and x = is_married in the entire dataset.

```{r}
correlation_overall = cor(adult$hours_per_week, adult$is_married)
cat("Correlation between hours_per_week and is_married (overall):", correlation_overall, "\n")

summary(lm(hours_per_week ~ is_married, data = adult))
```

Demonstrate that there is a strong negative correlation between y = hours_per_week and x = is_married among those who earn more than 50K (i.e., when conditioning the data so that the variable income = 1).

```{r}
adult_high_income = adult[income == 1, ]
correlation_high_income = cor(adult_high_income$hours_per_week, adult_high_income$is_married)
cat("Correlation between hours_per_week and is_married (income > 50K):", correlation_high_income, "\n")
summary(lm(hours_per_week ~ is_married, data = adult_high_income))
```

What is the name of the bias that embodies the discrepancy between these two results above?

Collider bias. Berkson's paradox is a specific type of collider bias which requires hours_per_week and is_married to be independent.

Make up a story as to why this bias paradox occurs. It is pure speculation!

One speculative story: Achieving a high income (>$50K) generally requires a significant contribution of either long work hours or other favorable circumstances (which might be associated with being married, e.g., specialization of labor in a household, spousal support allowing for riskier/higher-reward careers, or simply that marriage is correlated with age and experience which also correlate with income).


# Problem 5: Experimental Design

we will practice creating experimental designs on the lung cancer dataset. To do so we will drop the response (and hence the censoring) and imagine you see all these subjects at the same time.

```{r}
rm(list = ls())
lung = na.omit(survival::lung)
lung = lung[lung$ph.ecog < 3, ]
lung$time = NULL
lung$status = NULL
lung$inst = NULL
lung_male = lung[lung$sex == 1, ]
lung_female = lung[lung$sex == 2, ]
n = 160
X = as.matrix(rbind(lung_male[1 : (n / 2), ], lung_female[1 : (n / 2), ]))
rownames(X) = NULL
rm(lung, lung_male, lung_female)
head(X, 10)
```

You have p = 7 covariates for n = 160 subjects. The goal now is to randomize these subjects into pill (treatment) and placebo (control) arms. For each of the following exercises, create a matrix W of size n = 160 x R = 5,000 where each column is a randomized allocation drawn from the specific design.

Create W for the completely randomized design (CRD):

```{r}
R = 5000
Wcrd = matrix(NA, n, R)
for (r in 1 : R){
  Wcrd[, r] = sample(c(0,1), size = n, replace = TRUE, prob = c(0.5, 0.5))
}
```

Create W for the balanced completely randomized design (BCRD):

```{r}
Wbcrd = matrix(NA, n, R)
if (n %% 2 != 0) stop("n must be even for balanced design with n/2 in each group")
n_treat = n / 2
n_control = n / 2

for (r in 1 : R){
  Wbcrd[, r] = sample(c(rep(1, n_treat), rep(0, n_control)))
}
```

We will now practice generating allocations for restricted designs.

Create W for Fisher's Blocking design where we block on the covariate `sex`:

```{r}
Wblocking_sex = matrix(NA, n, R)
sex_col_idx = which(colnames(X) == "sex")
if (length(sex_col_idx) == 0) stop("Column 'sex' not found in X")

unique_sex_vals = unique(X[, sex_col_idx])
if (length(unique_sex_vals) != 2) stop("Expected two sex categories for blocking")

for (r in 1:R) {
  W_r = numeric(n)
  for (val in unique_sex_vals) {
    indices_block = which(X[, sex_col_idx] == val)
    n_block = length(indices_block)
    n_treat_block = floor(n_block / 2)
    n_control_block = n_block - n_treat_block
    assignments_block = sample(c(rep(1, n_treat_block), rep(0, n_control_block)))
    W_r[indices_block] = assignments_block
  }
  Wblocking_sex[, r] = W_r
}
```

Create W for Fisher's Blocking design where we block on the covariate `ph.ecog`:

```{r}
Wblocking_ph_ecog = matrix(NA, n, R)
ph_ecog_col_idx = which(colnames(X) == "ph.ecog")
if (length(ph_ecog_col_idx) == 0) stop("Column 'ph.ecog' not found in X")

unique_ph_ecog_vals = sort(unique(X[, ph_ecog_col_idx]))

for (r in 1:R) {
  W_r = numeric(n)
  for (val in unique_ph_ecog_vals) {
    indices_block = which(X[, ph_ecog_col_idx] == val)
    n_block = length(indices_block)
    if (n_block > 0) {
        n_treat_block = round(n_block / 2)
        n_control_block = n_block - n_treat_block
        if (n_block == 1) {
            assignments_block = sample(c(0,1), 1)
        } else {
            assignments_block = sample(c(rep(1, n_treat_block), rep(0, n_control_block)))
        }
        W_r[indices_block] = assignments_block
    }
  }
  Wblocking_ph_ecog[, r] = W_r
}
```

Create W for Fisher's Blocking design where we create B = 8 blocks on the covariate `age`:

```{r}
Wblocking_age = matrix(NA, n, R)
age_col_idx = which(colnames(X) == "age")
if (length(age_col_idx) == 0) stop("Column 'age' not found in X")
B = 8
age_quantiles = quantile(X[, age_col_idx], probs = seq(0, 1, length.out = B + 1), type=1) 
age_quantiles = unique(age_quantiles) 
if (length(age_quantiles) < 2) {
  age_blocks = cut(X[, age_col_idx], breaks = age_quantiles, include.lowest = TRUE, labels = FALSE)
} else {
  age_blocks = cut(X[, age_col_idx], breaks = age_quantiles, include.lowest = TRUE, labels = FALSE)
}

for (r in 1:R) {
  W_r = numeric(n)
  for (block_idx in unique(na.omit(age_blocks))) { # Iterate over actual block numbers
    indices_block = which(age_blocks == block_idx)
    n_block = length(indices_block)
    if (n_block > 0) {
        n_treat_block = round(n_block / 2)
        n_control_block = n_block - n_treat_block
        if (n_block == 1) {
            assignments_block = sample(c(0,1), 1)
        } else {
            assignments_block = sample(c(rep(1, n_treat_block), rep(0, n_control_block)))
        }
        W_r[indices_block] = assignments_block
    }
  }
  Wblocking_age[, r] = W_r
}
```

Create W for Students' rerandomization design retaining only the best 1% of allocations:

```{r}
Wrerand = matrix(NA, n, R)
X_std_rerand = scale(X) 
X_std_rerand = X_std_rerand[, apply(X_std_rerand, 2, var) > 1e-9, drop=FALSE]

calculate_balance_score = function(X_mat, assignment_vec) {
  score = 0
  n_t = sum(assignment_vec == 1)
  n_c = sum(assignment_vec == 0)
  if (n_t == 0 || n_c == 0) return(Inf)

  for (j in 1:ncol(X_mat)) {
    mean_t = mean(X_mat[assignment_vec == 1, j])
    mean_c = mean(X_mat[assignment_vec == 0, j])
    sd_overall = sd(X_mat[,j]) 
    if (sd_overall == 0) next 
    score = score + abs(mean_t - mean_c) / sd_overall
  }
  return(score)
}

R_large = R / 0.01
candidate_allocations = matrix(NA, n, R_large)
balance_scores = numeric(R_large)

n_treat_rerand = n / 2
n_control_rerand = n / 2
for (i in 1:R_large) {
  cand_alloc = sample(c(rep(1, n_treat_rerand), rep(0, n_control_rerand)))
  candidate_allocations[, i] = cand_alloc
  balance_scores[i] = calculate_balance_score(X_std_rerand, cand_alloc)
}

best_indices = order(balance_scores)[1:R]
Wrerand = candidate_allocations[, best_indices]
```

Create W for the pairwise matching (PM) design. To do so, we use the nonbipartite matching algorithm from the package `nbpMatching` as below:

```{r}
#create the distance matrix D using the standardized covariate values (see 342 demo code for the code that does this)

X_std_pm = scale(X)
X_std_pm = X_std_pm[, apply(X_std_pm, 2, var) > 1e-9, drop=FALSE]
D = as.matrix(dist(X_std_pm)) 

pacman::p_load(nbpMatching)
dist_obj = distancematrix(D) 
set.seed(1) 
match_obj = nonbimatch(dist_obj)
indicies_pairs = as.matrix(match_obj$matches[, c("Group1.Row", "Group2.Row")])


rm(D, dist_obj, match_obj) 
Wpm = matrix(NA, n, R)
for (r in 1:R) {
  W_r_pm = numeric(n)
  for (i in 1:nrow(indicies_pairs)) {
    pair = indicies_pairs[i, ]
    assignment_for_pair = sample(c(0, 1)) 
    W_r_pm[pair[1]] = assignment_for_pair[1]
    W_r_pm[pair[2]] = assignment_for_pair[2]
  }
  if (any(is.na(W_r_pm[unique(as.vector(indicies_pairs))]))) warning("Problem with PM assignments in pairs.")
  unmatched_indices = setdiff(1:n, unique(as.vector(indicies_pairs)))
  if(length(unmatched_indices) > 0) {
      W_r_pm[unmatched_indices] = sample(c(0,1), size=length(unmatched_indices), replace=TRUE)
  }
  Wpm[, r] = W_r_pm
}
```
For the problems below define "covariate balance" as the absolute standard deviation difference in a covariate between the subjects in the two arms. Hint: the `apply` function is your friend.

Demonstrate that the average covariate balance for the variable age is better in the w's from Fisher's blocking design on age than in the w's from CRD. 

```{r}
abs_std_mean_diff = function(covariate_values, allocation_vector) {
  if (length(unique(allocation_vector)) < 2) return(NA)
  if (sd(covariate_values) == 0) return(0) 
  
  mean_treat = mean(covariate_values[allocation_vector == 1], na.rm=TRUE)
  mean_control = mean(covariate_values[allocation_vector == 0], na.rm=TRUE)
  
  if (sum(allocation_vector == 1) == 0 || sum(allocation_vector == 0) == 0) return(NA)

  abs_diff = abs(mean_treat - mean_control)
  return(abs_diff / sd(covariate_values, na.rm=TRUE))
}

avg_cov_balance = function(W_matrix, covariate_data) {
  balance_scores_for_cov = apply(W_matrix, 2, function(alloc_col) {
    abs_std_mean_diff(covariate_data, alloc_col)
  })
  return(mean(balance_scores_for_cov, na.rm=TRUE))
}

avg_all_cov_balance = function(W_matrix, X_data) {
  num_covs = ncol(X_data)
  avg_balances_per_cov = numeric(num_covs)
  for (j in 1:num_covs) {
    avg_balances_per_cov[j] = avg_cov_balance(W_matrix, X_data[,j])
  }
  return(mean(avg_balances_per_cov, na.rm=TRUE))
}
```

Demonstrate that the average covariate balance for all variables in the w's from Student's rerandomization design is better than in the w's from BCRD. 

```{r}
age_data = X[, age_col_idx]

avg_balance_age_blocking = avg_cov_balance(Wblocking_age, age_data)
avg_balance_age_crd = avg_cov_balance(Wcrd, age_data)

cat("Average balance for age (Blocking on Age):", avg_balance_age_blocking, "\n")
cat("Average balance for age (CRD):", avg_balance_age_crd, "\n")
if (avg_balance_age_blocking < avg_balance_age_crd) {
  cat("Fisher's blocking on age achieved better average balance for age than CRD.\n")
} else {
  cat("Fisher's blocking on age did NOT achieve better average balance for age than CRD.\n")
}
```

Demonstrate that the average covariate balance for all variables in the w's from the PM design is better than in the w's from Student's rerandomization design. 

```{r}

avg_balance_all_rerand = avg_all_cov_balance(Wrerand, X)
avg_balance_all_bcrd = avg_all_cov_balance(Wbcrd, X)

cat("Average balance for all covariates (Rerandomization):", avg_balance_all_rerand, "\n")
cat("Average balance for all covariates (BCRD):", avg_balance_all_bcrd, "\n")

if (avg_balance_all_rerand < avg_balance_all_bcrd) {
  cat("Student's rerandomization achieved better average balance for all covariates than BCRD.\n")
} else {
  cat("Student's rerandomization did NOT achieve better average balance for all covariates than BCRD.\n")
}
```

# Problem 6: Fisher's Randomization Test

We load up data from a sociology experiment I ran with Prof Dana Weinberg to test racism, sexism and agism in the book publishing industry (see https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0267537 if you are interested).

```{r}
rm(list = ls())
pacman::p_load(data.table, R.utils)
racism_sexism_agism_experimental_data = fread("racism_sexism_agism_experimental_data.csv.bz2")
```

This experiment had many different arms: race, sex, age. For the purposes of this assignment, we will be looking just at the race arm, w = `tx_author_race` which we will binarize and the response will be y = `survey_willingness_to_pay` which we will convert into numeric. The original design was the CRD as the experiment was "sequential". You have experience with the CRD in the previous problem.

```{r}
wy = racism_sexism_agism_experimental_data[, .(w = tx_author_race, y = survey_willingness_to_pay)]
wy[, w := factor(as.numeric(ifelse(w == "White", 0, 1)))]
wy[, y := as.numeric(sub("\\$", "", y))]
head(wy)
```

Report the p-value for Fisher's randomization test for the strong null, i.e., H_0: y_i[w=0] = y_i[w=1] for all i. 

```{r}
y_obs = wy$y
w_obs = wy$w

if (length(unique(w_obs)) < 2) {
    stop("Treatment variable w_obs has fewer than two levels.")
}
if (sum(w_obs == 1) == 0 || sum(w_obs == 0) == 0) {
    stop("One of the treatment groups is empty.")
}

mean_y_treat_obs = mean(y_obs[w_obs == 1])
mean_y_control_obs = mean(y_obs[w_obs == 0])
T_obs = mean_y_treat_obs - mean_y_control_obs

num_permutations = 10000 
T_perm = numeric(num_permutations)

n_total_frt = nrow(wy)
n_treat_frt = sum(w_obs == 1)
set.seed(1)
for (i in 1:num_permutations) {
  w_perm = sample(w_obs) 
  mean_y_treat_perm = mean(y_obs[w_perm == 1])
  mean_y_control_perm = mean(y_obs[w_perm == 0])
  T_perm[i] = mean_y_treat_perm - mean_y_control_perm
}

p_value_frt = mean(abs(T_perm) >= abs(T_obs), na.rm=TRUE)

cat("Observed difference in means (T_obs):", T_obs, "\n")
cat("Fisher's Randomization Test p-value:", p_value_frt, "\n")
```
